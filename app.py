import os
# ** åœ¨å¯¼å…¥ä»»ä½•å…¶ä»–åº“ä¹‹å‰ï¼Œé¦–å…ˆè®¾ç½®ç¯å¢ƒå˜é‡ **
os.environ['TRANSFORMERS_NO_TENSORFLOW'] = 'true'

import streamlit as st
import torch
import torch.nn as nn
import pickle
from pathlib import Path
from PIL import Image, ImageDraw, ImageFont
import numpy as np
from torchvision import transforms
from torchvision.models import resnet50
from ultralytics import YOLO
from scipy.spatial.distance import cosine
import io
import easyocr
from sentence_transformers import SentenceTransformer

# --- é…ç½® ---
# ä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼Œç¡®ä¿é¡¹ç›®å¯ç§»æ¤
LIBRARY_FILE = Path('sku_library_multimodal.pkl')
FINETUNED_VISUAL_MODEL_PATH = Path('finetuned_resnet50_aug.pt')
YOLO_MODEL_PATH = 'runs/train/oriental_leaf_exp1/weights/best.pt'  # æ›´æ–°ä¸ºæˆ‘ä»¬åˆšåˆšè®­ç»ƒå¥½çš„æ¨¡å‹
TEXT_MODEL_NAME = 'paraphrase-multilingual-MiniLM-L12-v2'
# --- (ç»“æŸ) ---

# --- æ¨¡å‹å®šä¹‰ä¸åŠ è½½ ---

class EmbeddingNet(nn.Module):
    """è§†è§‰ç‰¹å¾æå–æ¨¡å‹ç»“æ„"""
    def __init__(self):
        super(EmbeddingNet, self).__init__()
        resnet = resnet50(weights=None)
        self.features = nn.Sequential(*list(resnet.children())[:-1])
    def forward(self, x):
        x = self.features(x)
        return torch.flatten(x, 1)

class VisualFeatureExtractor:
    """å°è£…äº†å¾®è°ƒåçš„è§†è§‰æ¨¡å‹"""
    def __init__(self, model_path):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model = EmbeddingNet().to(self.device)
        self.model.load_state_dict(torch.load(model_path, map_location=self.device))
        self.model.eval()
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
    def get_feature_vector(self, image):
        image = self.transform(image).unsqueeze(0).to(self.device)
        with torch.no_grad():
            embedding = self.model(image)
        return embedding.cpu().numpy().squeeze()

@st.cache_resource
def load_all_models():
    """ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰éœ€è¦çš„æ¨¡å‹"""
    print("æ­£åœ¨åŠ è½½æ‰€æœ‰æ¨¡å‹...")
    detector = YOLO(YOLO_MODEL_PATH)
    visual_extractor = VisualFeatureExtractor(FINETUNED_VISUAL_MODEL_PATH)
    text_encoder = SentenceTransformer(TEXT_MODEL_NAME)
    ocr_reader = easyocr.Reader(['ch_sim', 'en'])
    print("æ‰€æœ‰æ¨¡å‹åŠ è½½å®Œæˆã€‚")
    return detector, visual_extractor, text_encoder, ocr_reader

@st.cache_resource
def load_prototype_library():
    """åŠ è½½å¤šæ¨¡æ€åŸå‹çŸ¥è¯†åº“"""
    if not LIBRARY_FILE.exists():
        st.error(f"é”™è¯¯: å¤šæ¨¡æ€çŸ¥è¯†åº“ {LIBRARY_FILE} ä¸å­˜åœ¨ï¼è¯·å…ˆè¿è¡Œ build_prototype_library.pyã€‚")
        return None, None
    print("æ­£åœ¨åŠ è½½ã€å¤šæ¨¡æ€ã€‘SKUåŸå‹çŸ¥è¯†åº“...")
    with open(LIBRARY_FILE, 'rb') as f:
        prototype_library = pickle.load(f)
    class_names = list(prototype_library.keys())
    prototypes = np.array(list(prototype_library.values()))
    print("ã€å¤šæ¨¡æ€ã€‘SKUåŸå‹çŸ¥è¯†åº“åŠ è½½å®Œæˆã€‚")
    return class_names, prototypes

# --- ä¸»è¯†åˆ«å‡½æ•° ---
def run_recognition(image: Image.Image):
    detector, visual_extractor, text_encoder, ocr_reader = load_all_models()
    class_names, prototypes = load_prototype_library()

    if class_names is None: return None

    results = detector(image, classes=[39], verbose=False)
    detected_boxes = results[0].boxes.xyxy.cpu().numpy()

    if len(detected_boxes) == 0:
        st.warning("åœ¨å›¾ç‰‡ä¸­æœªèƒ½æ£€æµ‹åˆ°ä»»ä½•'ç“¶å­'ã€‚")
        return image

    draw = ImageDraw.Draw(image)
    try:
        font = ImageFont.truetype("Arial Unicode MS", 20)
    except IOError:
        font = ImageFont.load_default()

    for box in detected_boxes:
        cropped_image = image.crop(box)
        
        # --- **å¤šæ¨¡æ€ç‰¹å¾æå–** ---
        visual_embedding = visual_extractor.get_feature_vector(cropped_image)
        ocr_result = ocr_reader.readtext(np.array(cropped_image), detail=0, paragraph=True)
        ocr_text = " ".join(ocr_result)
        text_embedding = text_encoder.encode(ocr_text, convert_to_numpy=True)
        query_embedding = np.concatenate([visual_embedding, text_embedding])
        # --- **æå–ç»“æŸ** ---

        similarities = 1 - np.array([cosine(query_embedding, p) for p in prototypes])
        
        best_match_index = np.argmax(similarities)
        best_match_label = class_names[best_match_index]
        best_match_score = similarities[best_match_index]
        
        label_text = f"{best_match_label} ({best_match_score:.2f})"
        
        draw.rectangle(box, outline="cyan", width=3)
        text_bbox = draw.textbbox((box[0], box[1]), label_text, font=font)
        draw.rectangle(text_bbox, fill="cyan")
        draw.text((box[0], box[1]), label_text, fill="black", font=font)
        
    return image

# --- Streamlit åº”ç”¨ç•Œé¢ ---
st.set_page_config(layout="wide", page_title="å¤šæ¨¡æ€SKUè¯†åˆ«ç³»ç»Ÿ")
st.title("ğŸ‘‘ å¤šæ¨¡æ€SKUè¯†åˆ«ç³»ç»Ÿ (æœ€ç»ˆç‰ˆ)")
st.info("ä¸Šä¼ ä¸€å¼ è´§æ¶å›¾ç‰‡ï¼Œç³»ç»Ÿå°†ç»“åˆã€è§†è§‰ã€‘ä¸ã€æ–‡å­—ã€‘è¿›è¡Œè¯†åˆ«ã€‚")

uploaded_file = st.file_uploader("ä¸Šä¼ æ‚¨çš„è´§æ¶å›¾ç‰‡...", type=["jpg", "jpeg", "png"])

if uploaded_file is not None:
    image = Image.open(io.BytesIO(uploaded_file.getvalue())).convert("RGB")
    st.image(image, caption="æ‚¨ä¸Šä¼ çš„å›¾ç‰‡", use_column_width=True)
    
    if st.button("å¼€å§‹è¯†åˆ«", type="primary", use_container_width=True):
        with st.spinner('æ­£åœ¨è°ƒç”¨ã€å¤šæ¨¡æ€æ¨¡å‹ã€‘è¿›è¡Œè”åˆåˆ†æ...'):
            result_image = run_recognition(image)
        
        if result_image:
            st.success("è¯†åˆ«å®Œæˆï¼")
            st.image(result_image, caption="è¯†åˆ«ç»“æœ", use_column_width=True)
else:
    st.warning("è¯·ä¸Šä¼ ä¸€å¼ å›¾ç‰‡ä»¥å¼€å§‹ã€‚")
